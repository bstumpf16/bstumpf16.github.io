[
["index.html", "Statistics for Psychology Chapter 1 Introduction", " Statistics for Psychology B. Cameron Stumpf 2020-03-26 Chapter 1 Introduction I’m writing this because students seem to collectively suffer when it comes to learning statistics. Why is that? I want to fix it and make everyone’s lives easier. I love statistics! Why shouldn’t you? (Note to self: that sounds a little cheesy. Perhaps I should through in some more sass). No, seriously, I love stats. It’s actually kind of cool to see how real-life things can become numbers. It’s especially exciting when those numbers become graphics that can reveal things you wouldn’t have noticed otherwise. Anyways, I’m a student too. I get it. You probably aren’t even reading this. That’s okay, you have like 3 exams to study for and a paper to write. Best of luck to you! My goal is for this to be as minimalist and digestable as possible to spare you time. If you have any questions, comments, or opinions, don’t hesitate to reach out. Email me at bstumpf16@georgefox.edu. Special thanks to the professors who made me love math: Dr. Christina Aldrich, Dr. Christopher Koch, Dr. Sue O’Donnell, and Dr. Kelly Chang. Love you all! "],
["what-is-statistics.html", "Chapter 2 What is statistics? 2.1 Statistics in Psychology 2.2 Populations and Samples 2.3 Probability 2.4 Notation 2.5 Controversy", " Chapter 2 What is statistics? The word “statistics” is confusingly used in numerous ways. Statistics can refer to the field of mathematics or the analyses used by that field. A statistic is a quantitative summarization of data. In other words, a statistic is a number that describes observations. A mean is an example of a statistic. Statistical analyses are helpful because they allow us to understand the world on a deeper level. Whereas we are not entirely capable of looking at a spreadsheet of numbers and noticing trends and relationships, the process of statistical analysis can do just that. 2.1 Statistics in Psychology Psychology involves, the vast majority of the time, the study of people. The inherent problem is that such study is messy and prone to error. This is why the field of psychology has been criticized since its conception. However, statistics is like the housekeeper who cleans up the mess before guests arive. With it, psychology becomes more rigorous and scientific. One way we can think about the mess involved in the study of psychology is the uncertainty it creates. For example, when individuals fill out surveys, we don’t know with 100% certainty that they are being truthful. Unfortunately, if individuals lie in our studies, that takes away from our ability to draw meaningful conclusions from our results. Statistics uses mathematics and probability to quantify that uncertainty (see Hypothesis Testing). 2.2 Populations and Samples Information can generally come from two different places: either a population (such as all undergraduate students) or a sample (a selection of 100 undergraduate students). In many cases, population are too vast to gather information from each person. It would take a heck of a lot of time, money, and resources to gather information from all undergraduate students in the world. What we must often do is settle for a subset of those individuals that we are interested in. So, we choose just 100 undergraduate students. And what we might find is that nearly all of them slept, on average, two hours the previous night. That’s wild! We must have a serious problem on our hands! But wait! That was only 100 students and it just so happens that they all had an exam that morning. So, they stayed up pretty late studying. Can we generalize to all undergraduates? NO! In reality, undergraduates average six hours of sleep per night. Not ideal, but hey, at least it’s something. Remember how a statistic (like the mean) is used to describe things? Well, that’s partially true. A statistic can be used to describe a sample. When describing a population, the terminology changes. Instead, a parameter can be used to describe a population. Use the alliteration to help you remember (sample = statistic, population = paramter). The mean can actually be a population parameter as well as a sample statistic. When studying our sleepy undergraduates, their average of two hours of sleep per night was a sample statistic. The reality, that undergraduates average six hours of sleep per night, was a population parameter (though remember that we rarely know true parameters). They were pretty different! When we are making generalizations about populations from samples, we always run the risk of making mistakes. Let that both be a reason why statistical analyses are useful and why we should be cautious with them. Statistical analysis can help us quantify and account for our errors, but it can also yield errors itself. Kinda meta, right? It can be tricky to talk about. That’s part of why we’ve had hundreds of years of debate on the subject1. 2.3 Probability We use probability to quantify our uncertainty (or errors, or variability, or whichever word you choose). In doing so, we use words like “chance,” “randomness,” and “percentage.” In fact, we talk a whole lot about percentages. It’s good to be familiar with them. 2.4 Notation To succeed in statistical analysis, you need to be familiar with the notation used. Here are some common symbols: Symbol Meaning Example \\(\\large \\bar X\\) Mean \\(\\large \\bar X = \\frac{X_1 + X_2 + X_3}{3}\\) \\(\\large \\sum X\\) Sum \\(\\large \\sum X = X_1 + X_2 + X_3\\) \\(\\large \\hat X\\) Estimate \\(\\large \\hat s = \\sigma\\) 2.5 Controversy Statistical analysis is not without its faults. Over the years a number of statistics have actually led individuals farther from the truth. insert some kind of reference here to the debates↩ "],
["data.html", "Chapter 3 Data 3.1 What is data? 3.2 Types of data and Measurement 3.3 Organizing Data 3.4 Visualizing Data", " Chapter 3 Data “In God we trust, all others bring data.” —W. Edwards Deming “You can have data without information, but you cannot have information without data.” —Daniel Keys Moran 3.1 What is data? At its simplest, data is information. For the purposes of statistical analysis, the majority of data takes the form of numbers that represent things. The number 70 represents my height in inches. It is a data point. Context is important when it comes to data. If I had given you the number 70 with no other information, it wouldn’t be very useful. It takes on meaning when I tell you what it is meant to represent. Similarly, if I give you the number 22 with no other information, you can’t do much with that. It’s only when I tell you that the number represents my age that it is informative. Unfortunately, data (for our purposes) doesn’t just materialize out of thin air. We have to collect it. That’s where measurement comes in. 3.2 Types of data and Measurement Measurement is the process of quantifying something. By measuring my height, I assigned a number to myself that I can then use. I can share it with friends, compare it with others my age, or feel sad that it’s not a little higher. By the same process, I can measure the heights of 20 other people. I would then be collecting data. Now, measurement is a hot topic. And really, it’s just a theory. There’s no perfect consensus on the proper way to think about measurement. Sorry to introduce some gray area and subjectivity, but that’s where we’re at. Who said things had to be easy? I won’t bore you with the details (there are too many to cover here anyway), but interested readers can read more about it elsewhere2. Gray area is difficult to live in when it comes to mathematics, so at a point we must be decisive. One of the most widely accepted models of measurement in the field of psychology was published by Stanley Smith Stevens in 1946. Stevens classified measurement as falling into four types: nominal, ordinal, interval, or ratio. Each has specific characteristics and huge implications for how we handle data. 3.2.1 Nominal Nominal measurements are those that differ qualitatively from one another. They are typically used for identification and little else. There is a lot of freedom given to nominal measurements and consequently difficulty defining them precisely. It is almost easier to speak of what you cannot do with nominal measurements. You cannot order them in a way such that one is greater than another. Some examples of nominal measurements include: gender, ethnicity, nationality, and favorite flavor of ice-cream. 3.2.2 Ordinal Ordinal measurements are those that can be ordered (isn’t it nice when names make sense?). In other words, we can rank things measured on an ordinal scale. We are allowed to say that one thing is less than another thing. Beyond that, there’s not much we can do. This measurement tells us nothing about the distance between each measurement, only the order. Some examples of ordinal measurements include: letter grades, pain rating scales, personality questionaires, and race placement (1st place, 2nd place, etc.). 3.2.3 Interval Interval measurements are those that contain, you guess it, intervals! These measurements are such that a 1-unit decrease is equal to a 1-unit increase. Whereas ordinal scales told us nothing about the distance between measurements, interval scales finally allow us to do so. An important thing to note about interval measurements is that they do not have a true zero-point. It doesn’t make sense to speak of “zero temperature.” Some examples of interval measurements include Fahrenheit and hours on a clock. 3.2.4 Ratio Ratio measurements give us the most information out of the four types. Ratio scales are nearly identical to interval scales. The big difference is that ratio scales actually have a true zero point! Some examples of ratio measurements include: Kelvin, distance, age, and weight. 3.3 Organizing Data 3.4 Visualizing Data Numbers aren’t always intuitive way for humans to understand data. It often helps to see the data in the form of a graphic. Below are some examples of commonly used graphics. insert a footnote to direct students↩ "],
["measuring-central-tendency-and-variability.html", "Chapter 4 Measuring Central Tendency and Variability 4.1 Central Tendency 4.2 Variability 4.3 In Populations", " Chapter 4 Measuring Central Tendency and Variability Central tendency and variability are two of the most important ways used to describe data. 4.1 Central Tendency Measures of central tendency are those that attempt to describe where most of the data lies. Another way of thinking about is this: if you were to pull a number from a data set at random and guess which one you chose, how would you do so? The best way is to guess the value of one of the measures of central tendency (of which there are a few). 4.1.1 Common Measures of Central Tendency 4.1.1.1 Mean The mean is the number that is closest to all other numbers in a set. You are most likely familiar with it being described as the average. The mean is computed with the following formula: \\(\\Large Mean = \\frac{sum\\ of\\ scores}{number\\ of\\ scores}\\) The mean of a variable can be written as the variable with a bar over the top: \\(\\bar X\\). In mathematical notation, the formula looks like: \\(\\Large \\bar X = \\frac{\\sum X_i}{N}\\) 4.1.1.2 Median Imagine you took all the numbers in a set put them in order. The median is the middle number. In the set below, it is 31. \\[21,\\ 23,\\ 25,\\ 31,\\ 34,\\ 35,\\ 500\\] This is contrasted by the mean of this set, which would be: \\(Mean = \\frac{21 + 23 + 25 + 31 + 34 + 35 + 500}{7} = 95.57\\) The median is not as sensitive to more extreme numbers like the mean is. For this reason, you will often here people talk about median income instead of mean income (high-earners throw off the mean). Just to drive the point home further, we could change 500 to 1,000,000. The mean would then become 142881.3 but the median would stay at 31. 4.1.1.3 Mode The mode is the most occurring number is a set. Think m(ost)o(ccurring)de. It’s easy to calculate. Simply choose the number that occurs most often in the set. Similar to the median, the mode is not as sensitive to extreme scores. 4.2 Variability One number isn’t usually great at describing a data set. Sure, the mean might be 50, but what about the other numbers? That is where variability comes in. Variability describes how the scores vary or differ from one another. 4.2.1 Common Measures of Variability 4.2.1.1 Standard Deviation Standard deviation describes how much each score differs from the mean, on average. This is used in many statistical analyses and is worth being familiar with. \\(\\Large Standard\\ Deviation = \\sqrt{\\frac{(score\\ 1 - Mean)^2 + (score\\ 2 - Mean)^2 + ...}{number\\ of\\ scores}}\\) In mathematical notation, this looks like: \\(\\Large s = \\sqrt{\\frac{\\sum (X_i - \\bar X)^2}{N}}\\) Waaaaaaaaaait. I said the standard deviation was just an average, right? Why are things being squared and square rooted? If we were to sum all of the differences between each score and the mean, they would typically cancell out and equal zero (scores below the mean will give a negative difference, while scores above the mean will give a positive difference). So, they must be squared. But why are we square rooting things too?!?! That makes it easier to interpret. If we didn’t, we would be interpreting the average square distance from the mean. I don’t know about you, but I’m not smart enough for that to make intuitive sense. Instead, if we square root it, we can interpret it as the average distance from the mean, which makes much more sense. 4.2.1.2 Variance What if we didn’t take the square root in that last step when calculating the standard deviation? That would leave us with the variance, the average squared distance from the mean. \\(\\Large s^2 = \\frac{\\sum (X_i - \\bar X)^2}{N}\\) The variance and standard deviation are very similar and easily mixed up. Their relationship looks like this: \\(\\Large s = \\sqrt{s^2}\\) Be sure not to mistake them for eachtother (it’s really easy to do so but devastating to your analyses). 4.3 In Populations Measures of central tendency and variability can also be applied to populations. However, it is important to note once again how the notation differs when we are talking about a population instead of a sample. For the mean, we use the greek letter mu: \\(\\Large \\mu = \\frac{\\sum X_i}{N}\\) For the standard deviation, we use the greek letter sigma: \\(\\Large \\sigma = \\sqrt{\\frac{\\sum (X_i - \\mu)^2}{N}}\\) Notice also that instead of using \\(\\bar X\\) in this equation, we use the population mean, \\(\\mu\\). For the standard deviation, we use the greek letter sigma squared: \\(\\Large \\sigma^2 = \\frac{\\sum (X_i - \\mu)^2}{N}\\) "],
["estimating-and-sampling-distributions.html", "Chapter 5 Estimating and Sampling Distributions 5.1 Distribution of Sample Means", " Chapter 5 Estimating and Sampling Distributions 5.1 Distribution of Sample Means BRAINSTORM: Multiple types of distributions normal distribution features of a distribution estimating parameters degrees of freedom "],
["hypothesis-testing.html", "Chapter 6 Hypothesis Testing", " Chapter 6 Hypothesis Testing "],
["single-sample-z-test.html", "Chapter 7 Single Sample Z-Test 7.1 Example Setup", " Chapter 7 Single Sample Z-Test Usage: To check for a difference between a sampe mean and a population mean. Requirements: One variable that is interval/ratio, a population mean, and a population standard error. Steps to conducting an single samples z-test: Write out null and alternative hypotheses Determine critical z-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate z-value for your data A. You must first find the sample mean, population mean, and standard error Compare your calculated z-value to the critical z-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (Cohen’s d) 7.1 Example Setup 7.1.1 Hypotheses \\(\\Large H_0: \\mu = value\\) \\(\\Large H_1: \\mu \\neq value\\) 7.1.2 Critical z-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 7.1.3 Calculate z \\(\\Large N = number\\ of\\ scores\\) \\(\\Large \\bar X = \\frac{sum}{N}\\) \\(\\Large \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{N}}\\) \\(\\Large z = \\frac{\\bar X - \\mu}{\\sigma_{\\bar X}}\\) 7.1.4 Compare Observed z to CV If z is greater than the CV, reject the null hypothesis. If z is less than the CV, fail to reject the null hypothesis. 7.1.5 Calculate Effect Size \\(\\Large d = \\frac{\\bar X - \\mu}{\\sigma}\\) "],
["single-sample-t-test.html", "Chapter 8 Single Sample T-Test 8.1 Example Setup", " Chapter 8 Single Sample T-Test Usage: To check for a difference between a sampe mean and a population mean when the population error is unknown. Requirements: One variable that is interval/ratio and a population mean. Steps to conducting an single samples t-test: Write out null and alternative hypotheses Determine critical t-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate t-value for your data A. You must first find the sample mean, population mean, and estimated standard error Compare your calculated t-value to the critical t-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (Cohen’s d) Calculate confidence interval (CI). 8.1 Example Setup 8.1.1 Hypotheses \\(\\Large H_0: \\mu = value\\) \\(\\Large H_1: \\mu \\neq value\\) 8.1.2 Critical t-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 8.1.3 Calculate t \\(\\Large N = number\\ of\\ scores\\) \\(\\Large \\bar X = \\frac{sum}{N}\\) \\(\\Large \\hat s_\\bar X = \\frac{\\hat s}{\\sqrt{N}}\\) \\(\\Large t = \\frac{\\bar X - \\mu}{\\hat s_\\bar X}\\) 8.1.4 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 8.1.5 Calculate Effect Size \\(\\Large d = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s^2_p}}\\) 8.1.6 Calculate Confidence Interval (CI) \\(\\Large CI = \\bar X_\\ \\pm\\ (t_{critical}* \\hat{s}_{\\bar X})\\) "],
["independent-samples-t-test.html", "Chapter 9 Independent Samples T-Test 9.1 Example Setup", " Chapter 9 Independent Samples T-Test Usage: To check for a difference between two group means. Requirements: One variable with two levels and a second variable that is interval/ratio. Steps to conducting an independent samples t-test: Write out null and alternative hypotheses Determine critical t-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate t-value for your data A. You must first find the sample mean, population mean, and estimated standard error Compare your calculated t-value to the critical t-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (eta squared \\(\\eta^2\\) or Cohen’s d) Calculate confidence interval (CI). 9.1 Example Setup 9.1.1 Hypotheses \\(\\Large H_0: \\mu_1 - \\mu_2 = 0\\) \\(\\Large H_1: \\mu_1 - \\mu_2 \\neq 0\\) 9.1.2 Critical t-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 9.1.3 Calculate t \\(\\Large n_{group} = number\\ of\\ scores\\ in\\ group\\) \\(\\Large \\bar X_{group} = \\frac{sum}{n}\\) \\(\\Large SS_{group} = \\sum X^2_{group} - \\frac{(\\sum X_{group})^2}{n_{group}}\\) Calculate the previous three for both groups. \\(\\Large \\hat{s}_{\\bar X_1 - \\bar X_2} = \\sqrt{(\\frac{SS_1 + SS_2}{n_1 + n_2 - 2})(\\frac{1}{n_1} + \\frac{1}{n_2})}\\) \\(\\Large t = \\frac{\\bar X_1 - \\bar X_2}{\\hat{s}_{\\bar X_1 - \\bar X_2}}\\) 9.1.4 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 9.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{t^2}{t^2 + df}\\) \\(\\Large d = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s^2_p}}\\) 9.1.6 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar X_1 - \\bar X_2)\\ \\pm\\ (t_{critical}* \\hat{s}_{\\bar X_1 - \\bar X_2})\\) "],
["repeated-measures-t-test.html", "Chapter 10 Repeated Measures T-Test 10.1 Example Setup", " Chapter 10 Repeated Measures T-Test Usage: To check for a difference between means from the same group. Requirements: Two variables that are interval/ratio. Steps to conducting an repeated measures t-test: Write out null and alternative hypotheses Determine critical t-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate t-value for your data A. You must first find the mean difference and the estimated standard error. Compare your calculated t-value to the critical t-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (eta squared \\(\\eta^2\\) or Cohen’s d) Calculate confidence interval (CI). 10.1 Example Setup 10.1.1 Hypotheses \\(\\Large H_0: \\mu_a = \\mu_b\\) \\(\\Large H_1: \\mu_a \\neq \\mu_b\\) 10.1.2 Critical t-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 10.1.3 Calculate t \\(\\Large N = number\\ of\\ participants\\) \\(\\Large \\bar D = \\bar X_a - \\bar X_b\\) \\(\\Large SS_{D} = \\sum D^2 - \\frac{(\\sum D)^2}{N}\\) \\(\\Large \\hat{s}_D^2 = \\frac{SS_D}{N - 1}\\) \\(\\Large \\hat{s}_D = \\sqrt{\\hat{s}_D^2}\\) \\(\\Large \\hat{s}_{\\bar D} = \\frac{\\hat{s}_D}{\\sqrt{N}}\\) \\(\\Large t = \\frac{\\bar D}{\\hat{s}_{\\bar D}}\\) 10.1.4 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 10.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{t^2}{t^2 + df}\\) \\(\\Large d = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s^2_p}}\\) 10.1.6 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar D)\\ \\pm\\ (t_{critical}* \\hat{s}_{\\bar D})\\) "],
["one-way-between-subjects-anova.html", "Chapter 11 One-Way Between-Subjects ANOVA 11.1 Example Setup 11.2 Tips", " Chapter 11 One-Way Between-Subjects ANOVA Usage: To check for a difference between three or more group means. Requirements: Three or more variables that are interval/ratio. Note: This is basically the same as the independent groups t-test, but with more than two means. Steps to conducting an one-way between-subjects ANOVA: Write out null and alternative hypotheses. Determine critical F-value for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate F-value for your data. A. You must first find the sums of squares and mean squares. Compare your calculated F-value to the critical F-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (eta squared \\(\\eta^2\\)). Run post hoc tests if neccessary. Calculate confidence interval (CI). 11.1 Example Setup 11.1.1 Hypotheses \\(\\Large H_0: \\mu_a = \\mu_b = \\mu_c\\) \\(\\Large H_1: the\\ three\\ means\\ are\\ not\\ equal\\) 11.1.2 Critical F-Value (CV) \\(\\Large N = number\\ of\\ participants\\) \\(\\Large n_{group} = number\\ of\\ participants\\ in\\ group\\) \\(\\Large k = number\\ of\\ groups\\) \\(\\Large df_{between} = k - 1\\) \\(\\Large df_{within} = N - k\\) \\(\\Large df_{total} = N - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 11.1.3 Calculate F \\(\\Large T_{group} = \\sum X_{group}\\) \\(\\Large SS_{total} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{within} = \\sum X^2 - \\frac{\\sum T^2}{n}\\) \\(\\Large SS_{between} = \\frac{\\sum T^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large MS_{between} = \\frac{SS_{between}}{df_{between}}\\) \\(\\Large MS_{within} = \\frac{SS_{within}}{df_{within}}\\) \\(\\Large F = \\frac{MS_{between}}{MS_{within}}\\) 11.1.4 Compare Observed F to CV If F is greater than the CV, reject the null hypothesis. If F is less than the CV, fail to reject the null hypothesis. 11.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{SS_{between}}{SS_{total}}\\) 11.1.6 Post Hoc Testing Which means are different? Run Tukey’s HSD Test. \\(\\Large q = [check\\ table]\\) \\(\\Large CD = q\\sqrt{\\frac{MS_{within}}{n}}\\) Check to see if any of the differences between the means is greater than the CD. 11.1.7 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar X_a - \\bar X_b)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_a - \\bar X_c)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_b - \\bar X_c)\\ \\pm\\ CD\\) 11.2 Tips Draw a table like the one below to organize your calculations as you go. Source SS df MS F Between 50 2 25 12.5 Within 30 15 2 - Total 80 17 - - F is actually closely related to t: \\(\\Large F = t^2\\) "],
["one-way-repeated-measures-anova.html", "Chapter 12 One-Way Repeated Measures ANOVA 12.1 Example Setup 12.2 Tips", " Chapter 12 One-Way Repeated Measures ANOVA Usage: To check for a difference between three or more group means. Requirements: Three or more variables that are interval/ratio and within-subject design. Note: This is basically the same as the repeated measures t-test, but with more than two means. Steps to conducting an one-way repeated measures ANOVA: Write out null and alternative hypotheses. Determine critical F-value for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate F-value for your data. A. You must first find the sums of squares and mean squares. Compare your calculated F-value to the critical F-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (eta squared \\(\\eta^2\\)). Run post hoc tests if neccessary. Calculate confidence interval (CI). 12.1 Example Setup 12.1.1 Hypotheses \\(\\Large H_0: \\mu_a = \\mu_b = \\mu_c\\) \\(\\Large H_1: the\\ three\\ means\\ are\\ not\\ equal\\) 12.1.2 Critical F-Value (CV) \\(\\Large N = number\\ of\\ scores\\) \\(\\Large n = number\\ of\\ participants\\) \\(\\Large k = number\\ of\\ conditions\\) \\(\\Large df_{between\\ conditions} = k - 1\\) \\(\\Large df_{total} = N - 1\\) \\(\\Large df_{between\\ subjects} = n - 1\\) \\(\\Large df_{error} = (k-1)*(n-1)\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 12.1.3 Calculate F \\(\\Large T_{condition} = \\sum X_{condition}\\) \\(\\Large P_i = sum\\ of\\ scores\\ for\\ participant\\ i\\) \\(\\Large SS_{total} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{between\\ conditions} = \\frac{\\sum T^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{across\\ subjects} = \\frac{\\sum P_i^2}{k} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{error} = SS_{total} - SS_{between\\ conditions} - SS_{across\\ subjects}\\) \\(\\Large MS_{between\\ conditions} = \\frac{SS_{between\\ conditions}}{df_{between\\ conditions}}\\) \\(\\Large MS_{error} = \\frac{SS_{error}}{df_{error}}\\) \\(\\Large F = \\frac{MS_{between\\ conditions}}{MS_{error}}\\) 12.1.4 Compare Observed F to CV If F is greater than the CV, reject the null hypothesis. If F is less than the CV, fail to reject the null hypothesis. 12.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{SS_{between\\ conditions}}{SS_{between\\ conditions} + SS_{error}}\\) 12.1.6 Post Hoc Testing Which means are different? Run Tukey’s HSD Test. \\(\\Large q = [check\\ table]\\) \\(\\Large CD = q\\sqrt{\\frac{MS_{error}}{n}}\\) Check to see if any of the differences between the means is greater than the CD. 12.1.7 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar X_a - \\bar X_b)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_a - \\bar X_c)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_b - \\bar X_c)\\ \\pm\\ CD\\) 12.2 Tips Draw a table like the one below to organize your calculations as you go. Source SS df MS F Between Conditions 40 2 20 2 Error 20 2 10 - Across Subjects 80 2 - - Total 140 6 F is actually closely related to t: \\(\\Large F = t^2\\) "],
["two-way-anova.html", "Chapter 13 Two-Way ANOVA 13.1 Example Setup 13.2 Tips", " Chapter 13 Two-Way ANOVA Usage: To check for a difference between three or more group means with two sets of factors. Requirements: Three or more variables that are interval/ratio and two grouping variables. Steps to conducting an two-way ANOVA: Write out null and alternative hypotheses. A. Main effects of Factor A. B. Main effects of Factor B. C. Interaction of Factors A and B. Determine critical F-values for rejection of null hypotheses. A. Use degrees of freedom (df) and alpha level . B. One CV for each hypothesis. Calculate 3 F-values for your data. A. One for each hypothesis. Compare your calculated F-value to the critical F-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (eta squared \\(\\eta^2\\)). 13.1 Example Setup 13.1.1 Hypotheses For Factor A: \\(\\Large H_0: \\mu_{a_1} = \\mu_{a_2}\\) \\(\\Large H_1: \\mu_{a_1} \\neq \\mu_{a_2}\\) For Factor B: \\(\\Large H_0: \\mu_{b_1} = \\mu_{b_2}\\) \\(\\Large H_1: \\mu_{b_1} \\neq \\mu_{b_2}\\) For Interaction: \\(\\Large H_0: No\\ interaction\\) \\(\\Large H_1: Interaction\\ present\\) 13.1.2 Critical F-Value (CV) \\(\\Large N = number\\ of\\ participants\\) \\(\\Large n_{group} = number\\ of\\ participants\\ in\\ group\\) \\(\\Large k = number\\ of\\ groups\\) \\(\\Large df_{between} = k - 1\\) \\(\\Large df_{within} = (levels\\ of\\ A)*(levels\\ of\\ B)(n - 1)\\) \\(\\Large df_{total} = N - 1\\) \\(\\Large df_{A} = (levels\\ of\\ A) - 1\\) \\(\\Large df_{B} = (levels\\ of\\ B) - 1\\) \\(\\Large df_{interaction} = (df_A)*(df_B)\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 13.1.3 Calculate F \\(\\Large T_{group} = \\sum X_{group}\\) \\(\\Large SS_{total} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{within} = \\sum X^2 - \\frac{\\sum T^2}{n}\\) \\(\\Large SS_{between} = \\frac{\\sum T^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{A} = \\sum(\\frac{T^2_{row}}{n_{row}}) - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{B} = \\sum(\\frac{T^2_{col}}{n_{col}}) - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{AB} = SS_{between} - SS_A - SS_B\\) \\(\\Large MS_{A} = \\frac{SS_{A}}{df_{A}}\\) \\(\\Large MS_{B} = \\frac{SS_{B}}{df_{B}}\\) \\(\\Large MS_{AB} = \\frac{SS_{AB}}{df_{AB}}\\) \\(\\Large F_A = \\frac{MS_{A}}{MS_{within}}\\) \\(\\Large F_B = \\frac{MS_{B}}{MS_{within}}\\) \\(\\Large F_{AB} = \\frac{MS_{AB}}{MS_{within}}\\) 13.1.4 Compare Observed F-Value to CVs If F is greater than the CV, reject the null hypothesis. If F is less than the CV, fail to reject the null hypothesis. 13.1.5 Calculate Effect Size \\(\\Large \\eta^2_A = \\frac{SS_{A}}{SS_{A} + SS_{within}}\\) \\(\\Large \\eta^2_B = \\frac{SS_{B}}{SS_{B} + SS_{within}}\\) \\(\\Large \\eta^2_{AB} = \\frac{SS_{AB}}{SS_{AB} + SS_{within}}\\) 13.2 Tips Draw a table like the one below to organize your calculations as you go. Source SS df MS F Main Effect Factor A 40 1 40 2 Main Effect Factor B 40 1 40 2 Interaction 80 1 80 4 Within 140 7 20 Total 300 10 F is actually closely related to t: \\(\\Large F = t^2\\) "],
["correlation-and-regression.html", "Chapter 14 Correlation and Regression 14.1 Example Setup 14.2 Tips", " Chapter 14 Correlation and Regression Usage: To check for a relationship between variables. Requirements: Two or more variables that are interval/ratio. Steps to calculating a correlation coefficient: Write out null and alternative hypotheses. Calculate r-value (correlation coefficient). Determine critical t-value (yep!) for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate t-value. Compare your calculated t-value to the critical t-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (\\(r^2\\)). Estimate regression equation. Test significance of regression equation. A. Run an ANOVA. 14.1 Example Setup 14.1.1 Hypotheses \\(\\Large H_0: \\rho = 0\\) \\(\\Large H_1: \\rho \\neq 0\\) 14.1.2 Calculate r \\(\\Large n = number\\ of\\ participants\\) \\(\\Large SS_{X} = \\sum X^2 - \\frac{(\\sum X)^2}{n}\\) \\(\\Large SS_{Y} = \\sum Y^2 - \\frac{(\\sum Y)^2}{n}\\) \\(\\Large SP = \\sum XY - \\frac{\\sum X * \\sum Y}{n}\\) \\(\\Large r = \\frac{SP}{\\sqrt{SS_X * SS_Y}}\\) 14.1.3 Determine Critical t-Value (CV) \\(\\Large df = n - 2\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 14.1.4 Calculate t \\(\\Large t = \\frac{r}{\\sqrt{\\frac{(1 - r^2}{(n - 2)}})}\\) 14.1.5 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 14.1.6 Calculate Effect Size \\(\\Large r^2 = r * r\\) 14.1.7 Estimate Regression Equation Regression line equation: \\(\\hat Y = bX + a\\) \\(\\Large b = \\frac{SP}{SS_X}\\) \\(\\Large a = \\bar Y - (b*\\bar X)\\) \\(\\Large SEoE = \\sqrt{\\frac{(1 - r^2)*SS_Y}{n - 2}}\\) 14.1.8 Testing Significance of Regression 14.1.8.1 Hypotheses \\(\\Large H_0: b = 0\\) \\(\\Large H_1: b \\neq 0\\) 14.1.8.2 Run an ANOVA \\(\\Large df_{regression} = 1\\) \\(\\Large df_{residual} = n - 2\\) \\(\\Large SS_{regression} = r^2*SS_Y\\) \\(\\Large SS_{residual} = (1 - r^2)*SS_Y\\) \\(\\Large MS_{regression} = \\frac{SS_{regression}}{df_{regression}}\\) \\(\\Large MS_{residual} = \\frac{SS_{residual}}{df_{residual}}\\) \\(\\Large F = \\frac{MS_{regression}}{MS_{residual}}\\) 14.1.8.3 Check significance \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 14.2 Tips Remember that a t-test is used when hypothesis testing correlation coefficients! Remember that \\(df = n - 2\\) for tests of correlation coefficients! "],
["chi-square-test.html", "Chapter 15 Chi-Square Test 15.1 Example Setup 15.2 Chi-Square Goodness-of-Fit Test 15.3 Tips", " Chapter 15 Chi-Square Test Usage: To check for a relationship between variables. Requirements: Two nominal variables. Steps to running a chi-square test: Write out null and alternative hypotheses. Calculate \\(\\chi^2\\)-value. Determine critical \\(\\chi^2\\)-value (yep!) for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate \\(\\chi^2\\)-value. Compare your calculated \\(\\chi^2\\)-value to the critical \\(\\chi^2\\)-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (Cramer’s V). 15.1 Example Setup 15.1.1 Hypotheses \\(\\Large H_0: X\\ and\\ Y\\ are\\ unrelated\\) \\(\\Large H_1: X\\ and\\ Y\\ are\\ related\\) 15.1.2 Calculate Chi-Square \\(\\Large N = number\\ of\\ participants\\) \\(\\Large O = observed\\ values\\ in\\ each\\ cell\\) \\(\\Large CMF_{cell} = \\sum values\\ in\\ column\\) \\(\\Large RMF_{cell} = \\sum values\\ in\\ row\\) \\(\\Large E_{cell} = (\\frac{CMF_{cell}}{N})*(RMF_{cell})\\) \\(\\Large \\chi^2 = \\sum \\frac{(O_{cell} - E_{cell})^2}{E_{cell}}\\) 15.1.3 Determine Critical Chi-Square Value (CV) \\(\\Large df = (rows - 1)(columns - 1)\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 15.1.4 Compare Observed Chi-Square to CV If \\(\\chi^2\\) is greater than the CV, reject the null hypothesis. If \\(\\chi^2\\) is less than the CV, fail to reject the null hypothesis. 15.1.5 Calculate Effect Size \\(\\Large L = number\\ of\\ levels\\ of\\ variable\\ with\\ least\\ levels\\) \\(\\Large V = \\sqrt{\\frac{\\chi^2}{N*(L - 1)}}\\) 15.2 Chi-Square Goodness-of-Fit Test This involves testing the relative frequencies of a single nominal variables. The setup is similar to a general \\(\\chi^2\\) test. The main difference is the calculation of the expected frequencies. \\(\\Large rf_{category} = given\\) Or, if it is not given and all categories are expected to be proportional: \\(\\Large rf_{category} = \\frac{N}{number\\ of\\ categories}\\) \\(\\Large E_{category} = (rf_{category})*(N)\\) 15.3 Tips \\(\\chi^2\\) is pronounced K-Y, NOT CH-Y. Remember not to get \\(\\chi^2\\) mixed up with X. Draw a table like the one below to organize your calculations as you go. Cell \\(O\\) \\(E\\) \\(O - E\\) \\((O - E)^2\\) \\(\\frac{(O - E)^2}{E}\\) Religious Males 20 23.08 -3.08 9.49 0.41 Religious Females 30 26.92 3.08 9.49 0.35 Non-Religious Males 10 6.92 3.08 9.49 1.37 Non-Religious Females 5 8.08 -3.08 9.49 1.17 \\(\\chi^2\\) 3.30 "],
["references.html", "Chapter 16 References", " Chapter 16 References Jaccard, J. &amp; Becker, M. A. (2002). Statistics for the behavioral sciences. Wadsworth Group. Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677-680. "]
]

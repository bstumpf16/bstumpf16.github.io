[
["index.html", "Statistics for Psychology Chapter 1 Introduction", " Statistics for Psychology B. Cameron Stumpf 2020-03-27 Chapter 1 Introduction I’m writing this because students seem to collectively suffer when it comes to learning statistics. Why is that? I want to fix it and make everyone’s lives easier. I love statistics! Why shouldn’t you? (Note to self: that sounds a little cheesy. Perhaps I should through in some more sass). No, seriously, I love stats. It’s actually kind of cool to see how real-life things can become numbers. It’s especially exciting when those numbers become graphics that can reveal things you wouldn’t have noticed otherwise. Anyways, I’m a student too. I get it. You probably aren’t even reading this. That’s okay, you have like 3 exams to study for and a paper to write. Best of luck to you! My goal is for this to be as minimalist and digestable as possible to spare you time. If you have any questions, comments, or opinions, don’t hesitate to reach out. Email me at bstumpf16@georgefox.edu. Special thanks to the professors who made me love math: Dr. Christina Aldrich, Dr. Christopher Koch, Dr. Sue O’Donnell, and Dr. Kelly Chang. Love you all! "],
["what-is-statistics.html", "Chapter 2 What is statistics? 2.1 Statistics in Psychology 2.2 Populations and Samples 2.3 Probability 2.4 Notation 2.5 Controversy", " Chapter 2 What is statistics? The word “statistics” is confusingly used in numerous ways. Statistics can refer to the field of mathematics or the analyses used by that field. A statistic is a quantitative summarization of data. In other words, a statistic is a number that describes observations. A mean is an example of a statistic. Statistical analyses are helpful because they allow us to understand the world on a deeper level. Whereas we are not entirely capable of looking at a spreadsheet of numbers and noticing trends and relationships, the process of statistical analysis can do just that. 2.1 Statistics in Psychology Psychology involves, the vast majority of the time, the study of people. The inherent problem is that such study is messy and prone to error. This is why the field of psychology has been criticized since its conception. However, statistics is like the housekeeper who cleans up the mess before guests arive. With it, psychology becomes more rigorous and scientific. One way we can think about the mess involved in the study of psychology is the uncertainty it creates. For example, when individuals fill out surveys, we don’t know with 100% certainty that they are being truthful. Unfortunately, if individuals lie in our studies, that takes away from our ability to draw meaningful conclusions from our results. Statistics uses mathematics and probability to quantify that uncertainty (see Hypothesis Testing). 2.2 Populations and Samples Information can generally come from two different places: either a population (such as all undergraduate students) or a sample (a selection of 100 undergraduate students). In many cases, population are too vast to gather information from each person. It would take a heck of a lot of time, money, and resources to gather information from all undergraduate students in the world. What we must often do is settle for a subset of those individuals that we are interested in. So, we choose just 100 undergraduate students. And what we might find is that nearly all of them slept, on average, two hours the previous night. That’s wild! We must have a serious problem on our hands! But wait! That was only 100 students and it just so happens that they all had an exam that morning. So, they stayed up pretty late studying. Can we generalize to all undergraduates? NO! In reality, undergraduates average six hours of sleep per night. Not ideal, but hey, at least it’s something. Remember how a statistic (like the mean) is used to describe things? Well, that’s partially true. A statistic can be used to describe a sample. When describing a population, the terminology changes. Instead, a parameter can be used to describe a population. Use the alliteration to help you remember (sample = statistic, population = paramter). The mean can actually be a population parameter as well as a sample statistic. When studying our sleepy undergraduates, their average of two hours of sleep per night was a sample statistic. The reality, that undergraduates average six hours of sleep per night, was a population parameter (though remember that we rarely know true parameters). They were pretty different! When we are making generalizations about populations from samples, we always run the risk of making mistakes. Let that both be a reason why statistical analyses are useful and why we should be cautious with them. Statistical analysis can help us quantify and account for our errors, but it can also yield errors itself. Kinda meta, right? It can be tricky to talk about. That’s part of why we’ve had hundreds of years of debate on the subject1. 2.3 Probability We use probability to quantify our uncertainty (or errors, or variability, or whichever word you choose). In doing so, we use words like “chance,” “randomness,” and “percentage.” In fact, we talk a whole lot about percentages. It’s good to be familiar with them. 2.4 Notation To succeed in statistical analysis, you need to be familiar with the notation used. Here are some common symbols: Symbol Meaning Example \\(\\large \\bar X\\) Mean \\(\\large \\bar X = \\frac{X_1 + X_2 + X_3}{3}\\) \\(\\large \\sum X\\) Sum \\(\\large \\sum X = X_1 + X_2 + X_3\\) \\(\\large \\hat X\\) Estimate \\(\\large \\hat s = \\sigma\\) 2.5 Controversy Statistical analysis is not without its faults. Over the years a number of statistics have actually led individuals farther from the truth. insert some kind of reference here to the debates↩ "],
["data.html", "Chapter 3 Data 3.1 What is data? 3.2 Types of Data and Measurement 3.3 Distributions", " Chapter 3 Data “In God we trust, all others bring data.” —W. Edwards Deming “You can have data without information, but you cannot have information without data.” —Daniel Keys Moran 3.1 What is data? At its simplest, data is information. For the purposes of statistical analysis, the majority of data takes the form of numbers that represent things. The number 70 represents my height in inches. It is a data point. Context is important when it comes to data. If I had given you the number 70 with no other information, it wouldn’t be very useful. It takes on meaning when I tell you what it is meant to represent. Similarly, if I give you the number 22 with no other information, you can’t do much with that. It’s only when I tell you that the number represents my age that it is informative. Unfortunately, data (for our purposes) doesn’t just materialize out of thin air. We have to collect it. That’s where measurement comes in. 3.2 Types of Data and Measurement Measurement is the process of quantifying something. By measuring my height, I assigned a number to myself that I can then use. I can share it with friends, compare it with others my age, or feel sad that it’s not a little higher. By the same process, I can measure the heights of 20 other people. I would then be collecting data. Now, measurement is a hot topic. And really, it’s just a theory. There’s no perfect consensus on the proper way to think about measurement. Sorry to introduce some gray area and subjectivity, but that’s where we’re at. Who said things had to be easy? I won’t bore you with the details (there are too many to cover here anyway), but interested readers can read more about it elsewhere2. Gray area is difficult to live in when it comes to mathematics, so at a point we must be decisive. One of the most widely accepted models of measurement in the field of psychology was published by Stanley Smith Stevens in 1946. Stevens classified measurement as falling into four types: nominal, ordinal, interval, or ratio. Each has specific characteristics and huge implications for how we handle data. 3.2.1 Nominal Nominal measurements are those that differ qualitatively from one another. They are typically used for identification and little else. There is a lot of freedom given to nominal measurements and consequently difficulty defining them precisely. It is almost easier to speak of what you cannot do with nominal measurements. You cannot order them in a way such that one is greater than another. Some examples of nominal measurements include: gender, ethnicity, nationality, and favorite flavor of ice-cream. 3.2.2 Ordinal Ordinal measurements are those that can be ordered (isn’t it nice when names make sense?). In other words, we can rank things measured on an ordinal scale. We are allowed to say that one thing is less than another thing. Beyond that, there’s not much we can do. This measurement tells us nothing about the distance between each measurement, only the order. Some examples of ordinal measurements include: letter grades, pain rating scales, personality questionaires, and race placement (1st place, 2nd place, etc.). 3.2.3 Interval Interval measurements are those that contain, you guess it, intervals! These measurements are such that a 1-unit decrease is equal to a 1-unit increase. Whereas ordinal scales told us nothing about the distance between measurements, interval scales finally allow us to do so. An important thing to note about interval measurements is that they do not have a true zero-point. It doesn’t make sense to speak of “zero temperature.” Some examples of interval measurements include Fahrenheit and hours on a clock. 3.2.4 Ratio Ratio measurements give us the most information out of the four types. Ratio scales are nearly identical to interval scales. The big difference is that ratio scales actually have a true zero point! Some examples of ratio measurements include: Kelvin, distance, age, and weight. 3.3 Distributions Data is usually distributed among a number of values. This can be examined in a number of ways. 3.3.1 Frequencies Distributions The frequency, f, of data refers to the amount of times each value occurs in the set. The following table shows the set of frequencies for a data set. Grades f rf crf A 15 .20 1.0 B 25 .34 .80 C 20 .27 .46 D 11 .15 .19 F 3 .04 .04 \\(\\Large n = sum\\ of\\ all\\ frequencies\\) The letter n tells us the number of scores, participants, or the sum of the frequencies. In this case, n refers to all three and equals 74. The relative frequencies, rf, can be interpreted as percentages. For example, 20% of the participants recieved an A. The cumulative relative frequencies, crf, tell us the frequency of that value and all values lower than that value. For example, 46% of the participants recieved at least a C. 3.3.2 Graphical Distributions A bar graph is a visual representation of the frequencies in a data set when the variable is categorical. A histograms is a visual representation of the frequencies in a data set when the variable is categorical. Histograms look similar to a bar graph. Notice that instead, the bars on a histogram are touching. Each of our grades in the last example corresponded to an actual percentage. Let’s look at the distribution of those percentages. 3.3.3 Probability Distributions An interesting feature of histograms is that they can also serve as probability distributions. Probability distributions are those that show the chances of selecting a value at random from a set. We can plot an approximated line on our histogram to demonstrate a probability distribution. The higher the line, the higher the probability at that value. If we were to select a random grade from this set, this graph suggests that we would likely choose one between .70 and 1. The line is a lot lower near .3, suggesting that such a score is unlikely. 3.3.4 The Normal Distribution Distributions of actual data are called empirical distributions. We also talk about theoretical distributions, which have certain shapes and properties. The normal distribution is a theoretical distribution that looks like a bell. For this reason, it is often called a bell curve. Normal distributions have a clump of scores in the middle (near the mean) that dwindle out in tails (moving away from the mean). Here is an example of normally distributed grades: Statisticians will often talk about the central limit theorem, which is a fancy term that basically means: the larger your sample size, the closer it will get to normally distribution. The example above is a distribution of 800,000 scores. That is a lot. Contrast that with a distribution from only 50 scores: Sure, it’s still somewhat normal, but it’s nowhere near as nice and normal as the previous example. insert a footnote to direct students↩ "],
["describing-data.html", "Chapter 4 Describing Data 4.1 Central Tendency 4.2 Variability 4.3 In Populations 4.4 Skewness and Kurtosis 4.5 Relative Scores", " Chapter 4 Describing Data 4.1 Central Tendency Measures of central tendency are those that attempt to describe where most of the data lies. Another way of thinking about is this: if you were to pull a number from a data set at random and guess which one you chose, how would you do so? The best way is to guess the value of one of the measures of central tendency (of which there are a few). 4.1.1 Common Measures of Central Tendency 4.1.1.1 Mean The mean is the number that is closest to all other numbers in a set. You are most likely familiar with it being described as the average. The mean is computed with the following formula: \\(\\Large Mean = \\frac{sum\\ of\\ scores}{number\\ of\\ scores}\\) The mean of a variable can be written as the variable with a bar over the top: \\(\\bar X\\). In mathematical notation, the formula looks like: \\(\\Large \\bar X = \\frac{\\sum X_i}{N}\\) 4.1.1.2 Median Imagine you took all the numbers in a set put them in order. The median is the middle number. In the set below, it is 31. \\[21,\\ 23,\\ 25,\\ 31,\\ 34,\\ 35,\\ 500\\] This is contrasted by the mean of this set, which would be: \\(Mean = \\frac{21 + 23 + 25 + 31 + 34 + 35 + 500}{7} = 95.57\\) The median is not as sensitive to more extreme numbers like the mean is. For this reason, you will often here people talk about median income instead of mean income (high-earners throw off the mean). Just to drive the point home further, we could change 500 to 1,000,000. The mean would then become 142881.3 but the median would stay at 31. 4.1.1.3 Mode The mode is the most occurring number is a set. Think m(ost)o(ccurring)de. It’s easy to calculate. Simply choose the number that occurs most often in the set. Similar to the median, the mode is not as sensitive to extreme scores. 4.2 Variability One number isn’t usually great at describing a data set. Sure, the mean might be 50, but what about the other numbers? That is where variability comes in. Variability describes how the scores vary or differ from one another. 4.2.1 Common Measures of Variability 4.2.1.1 Range The range of data is defined by the lowest score and the highest score, as well as the difference between them. Looking at the range of data can be helpful because it intuitively shows how spread out the data is. 4.2.1.2 Standard Deviation Standard deviation describes how much each score differs from the mean, on average. This is used in many statistical analyses and is worth being familiar with. \\(\\Large Standard\\ Deviation = \\sqrt{\\frac{(score\\ 1 - mean)^2 + (score\\ 2 - mean)^2 + ...}{number\\ of\\ scores}}\\) In mathematical notation, this looks like: \\(\\Large s = \\sqrt{\\frac{\\sum (X_i - \\bar X)^2}{N}}\\) Waaaaaaaaaait. I said the standard deviation was just an average, right? Why are things being squared and square rooted? If we were to sum all of the differences between each score and the mean, they would typically cancell out and equal zero (scores below the mean will give a negative difference, while scores above the mean will give a positive difference). So, they must be squared. But why are we square rooting things too?!?! That makes it easier to interpret. If we didn’t, we would be interpreting the average square distance from the mean. I don’t know about you, but I’m not smart enough for that to make intuitive sense. Instead, if we square root it, we can interpret it as the average distance from the mean, which makes much more sense. 4.2.1.3 Variance What if we didn’t take the square root in that last step when calculating the standard deviation? That would leave us with the variance, the average squared distance from the mean. \\(\\Large s^2 = \\frac{\\sum (X_i - \\bar X)^2}{N}\\) The variance and standard deviation are very similar and easily mixed up. Their relationship looks like this: \\(\\Large s = \\sqrt{s^2}\\) Be sure not to mistake them for eachtother (it’s really easy to do so but devastating to your analyses). 4.3 In Populations Measures of central tendency and variability can also be applied to populations. However, it is important to note once again how the notation differs when we are talking about a population instead of a sample. For the mean, we use the greek letter mu: \\(\\Large \\mu = \\frac{\\sum X_i}{N}\\) For the standard deviation, we use the greek letter sigma: \\(\\Large \\sigma = \\sqrt{\\frac{\\sum (X_i - \\mu)^2}{N}}\\) Notice also that instead of using \\(\\bar X\\) in this equation, we use the population mean, \\(\\mu\\). For the standard deviation, we use the greek letter sigma squared: \\(\\Large \\sigma^2 = \\frac{\\sum (X_i - \\mu)^2}{N}\\) 4.4 Skewness and Kurtosis Remember how we described certain distributions as normal? There are also ways to describe how a distribution deviates from normality. Skewness occurs when one tail of a distribution is longer than the other. Positive skew is when the right tail is longer, and negative skew is when the left tail is longer. Kurtosis occurs when a distribution is flatter or more pointed that normal. A pointed distribution is leptokurtic, while a flatter distribution is platykurtic. 4.5 Relative Scores Sometimes it is beneficial for us to evaluate a score in the context of all of the other scores. 4.5.1 Percentiles One way to compare scores the all others is with percentiles. Percentiles note the percentage of other scores that a particular score is above. Does that sound familiar? It’s very similar to the idea of cumulative relative frequencies. We can refer to the 90th percentile in a data set, which indicates the score at which 90% of other scores fall below. We can also say that someone scored in the 90th percentile, which indicates that they scored above 90% of the other scores. Percentiles are often used in standardized testing. 4.5.2 Standard Scores In addition to percentiles, standard scores can also be used to evaluate a score in the context of the other scores. A standard score tells us how many standard deviations above (if it is positive) or below (if it is negative) the mean a particular score is. \\(\\Large Standard\\ Score = \\frac{score - mean}{standard\\ deviation}\\) In mathematical notation, the formula looks like: \\(\\Large Standard\\ Score = \\frac{X - \\bar X}{s}\\) "],
["estimation.html", "Chapter 5 Estimation 5.1 Sampling Distribution 5.2 Estimating the Mean 5.3 Estimating the Variance 5.4 Estimating the Standard Deviation 5.5 Estimating Standard Error 5.6 Degrees of Freedom 5.7 Keeping Everything Straight", " Chapter 5 Estimation Most of the time, we don’t know the population parameters we are interested in and it would be unrealistic to collect data from everyone in the population. Estimation allows us to take a sample from that population and use that sample to make an educated guess about the population. This process is also referred to as inferential statistics. This process usually involves taking a sample statistic and using that to estimate the population parameter. The primary issue here is whether the sample accurately reflects the population or not. Every sample could be different, right? So how do we know which sample to trust? The short answer: we don’t. Statisticians wish that such certainty was possible. But then again, the whole concept of statistical analysis is to figure out how to handle our uncertainty. The concept of sampling distributions helps us do this. 5.1 Sampling Distribution A sampling Distribution of a statistic is a distribution of all possible values of that statistic from all possible samples of a population. This is not to be confused with the sample distribution, which is the distribution of scores in a sample. Say we have a population of 100. And say we took a sample of 30. That means that there are 70 individuals who were not sampled. Couldn’t we have just as easily chosen a sample of a different 30 individuals? In fact, there are 29,372,339,821,610,944,823,963,760 different possible samples of we could have taken. Now, each of those samples could have given us a different mean. if we had actually taken all possible samples, and taken all possible means, then we would have a large set of numbers. And what can we do with sets of numbers? Graph distributions! More specifically, we can create a sampling distribution of the mean. Below is a distribution of means from 10,000 different samples of size 30 from the population of 100. Hey, it looks like a normal distribution! That’s nifty. 5.2 Estimating the Mean This distribution has a pretty cool feature. The mean of sample means is 70.09, and the actual population mean is 70.09! Using this distribution, statisticians can estimate the population mean pretty accurately. But it’s also important to notice how some of the sample means were as low as 65. It may not have been many, but it shows that there is always a chance that a sample could lead to an incorrect estimate. Because of these properties, the sample mean is known as an unbiased estimator. That is, it approximates the population mean precisely. 5.3 Estimating the Variance Unlike the sample mean, the sample variance is a biased estimator. That is, it is consistently lower than the actual population variance. Sounds like we need a new formula! Remember the formula for calculating sample variance: \\(\\Large s^2 = \\frac{\\sum (X_i - \\bar X)^2}{N}\\) To calculate an estimate of the population variance, we alter the formula a bit: \\(\\Large \\hat s^2 = \\frac{\\sum (X_i - \\bar X)^2}{N-1}\\) Notice the tiny “hat” on the \\(\\hat s^2\\). That little hat tells us that this is an estimate of the population variance This is distinguished from the sample variance and the actual population variance. 5.4 Estimating the Standard Deviation One of the ways to calculate the standard deviation in a sample is to take the square root of the variance. Fortunately, the same applies when estimating the population standard deviation (less formulas to remember!). You can simply use the following: \\(\\Large \\hat s = \\sqrt{\\hat s^2}\\) Don’t get \\(s\\) and \\(\\hat s\\) mixed up! They look very similar but are quite different. 5.5 Estimating Standard Error In addition to calculating the mean of the sampling distribution of the mean, we can also calculate the standard deviation of the distribution. When this is done for the sampling distribution of the mean, this is called the standard error of the mean. A standard error of zero means that the sample mean estimates the population mean perfectly, while higher standard errors indicate a lack of reliability in the estimation of the population mean. The standard error of the mean is calculated with the following forumla: \\(\\Large \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{N}}\\) But remember! We rarely know the population variance. So, we usually have to use this formula: \\(\\Large \\hat s_{\\bar X} = \\frac{\\hat s}{\\sqrt{N}}\\) It’s important not to get \\(\\hat s_{\\bar X}\\) mixed up with \\(s\\), \\(\\hat s\\), \\(s^2\\), and \\(\\hat s^2\\)! They can get pretty confusing. 5.6 Degrees of Freedom Technically speaking, degrees of freedom tell us how many things in a set are independent of eachother. Now if you’re like me, that definition kinda makes no sense. So I’ll try to explain. Suppose you have a set of nine numbers. However, you only know the mean and eight of those numbers. Using basic algebra, you can solve for the ninth number. In other words, eight of those nine numbers were free to take on any value. But once they did, that ninth number could only take on one value. We could say that there were eight degrees of freedom there (because eight numbers were free to vary). 5.7 Keeping Everything Straight Here’s a table to help you remember the differences between population parameters, sample statistics, and sample estimates of population parameters: Statistic Population Sample Sample Estimate of the Popoulation Mean \\(\\Large \\mu = \\frac{\\sum X_i}{N}\\) \\(\\Large \\bar X = \\frac{\\sum X_i}{N}\\) \\(\\Large \\bar X = \\frac{\\sum X_i}{N}\\) Variance \\(\\Large \\sigma^2 = \\frac{\\sum (X_i - \\mu)^2}{N}\\) \\(\\Large s^2 = \\frac{\\sum (X_i - \\bar X)^2}{N}\\) \\(\\Large \\hat s^2 = \\frac{\\sum (X_i - \\bar X)^2}{N-1}\\) Standard Deviation \\(\\Large \\sigma = \\sqrt{\\frac{\\sum (X_i - \\mu)^2}{N}}\\) \\(\\Large s = \\sqrt{\\frac{\\sum (X_i - \\bar X)^2}{N}}\\) \\(\\Large \\hat s = \\sqrt{\\frac{\\sum (X_i - \\bar X)^2}{N-1}}\\) "],
["hypothesis-testing.html", "Chapter 6 Hypothesis Testing 6.1 Stating Hypotheses 6.2 Rejecting the Null 6.3 Alpha Levels 6.4 Type I &amp; Type II Errors 6.5 Power and Sample Size 6.6 Directional and Non-Directional", " Chapter 6 Hypothesis Testing Hypothesis testing is the process of using data to evaluate a claim. 6.1 Stating Hypotheses There are generally two hypotheses of interest: the alternative and the null. The alternative hypothesis is the researchers claim, often that a treatment has an effect on the participants. The null hypothesis states the opposite, that there is no effect. They are written like this: \\(\\Large H_0: \\mu_1 - \\mu_2 = 0\\) \\(\\Large H_1: \\mu_1 - \\mu_2 \\neq 0\\) In the above, the null hypothesis (\\(H_0\\)) states that one mean minus a secon mean equals zero. In other words, the two means are equal. If one population was treated differently than the other, the null hypothesis suggests that the treatment had no effect. The alternative hypothese (\\(H_1\\)) states the opposite: that the two means differ from one another. In other words, the treatment had an effect. This is a vast oversimplification but it gets the point across. The hypothesis testing process aids us in decided which hypothesis to choose. Based on the evidence provided by the data, we can reject the null hypothesis. That makes sense, right? If our data disagrees that two means are equal, we reject the hypothesis that two means are equal. Pretty straightforward. But here is where it gets complicated (I totally understand if you have to read this over a couple times. Or 20). Even if we reject the null hypothesis, that does not mean we can accept the alternative hypothesis. And to complicate it even further, we can never accept that null hypothesis either. We can only fail to reject the null hypothesis. How’s that for a triple negative? Interested readers can read more elswhere3, but here is the bottomline: hypothesis testing can only help us to reject or fail to reject the null hypothesis. Nothing more, nothing less. 6.2 Rejecting the Null To reject the null hypothesis, your data must disagree with what the null hypothesis claims to be true. But we know that our methods are prone to error. Think about the following example. Your friend says that the class average on the exam was 90%. You disagree with him and think it’s something else. You state your hypotheses: \\(\\Large H_0: \\mu = 90%\\) \\(\\Large H_1: \\mu \\neq 90%\\) And say we asked 5 of the 20 students what they got on the test. The average of their scores was 88%. That’s different from 90%, but not by much. How much different does it have to be for us to reject the null hypothesis? Well, that typically has to do with how spread out the data is. If the scores varied by 20% on average, then a 2% difference might seem trivial. Your friend is probably right. But say that all the students in the sample scored 88%. There is little variation there, so the 2% difference might be meaninful. 6.3 Alpha Levels It’s safe to say that, in general, researchers want to reject the null hypothesis. They like to show that things are related or have influence on other things. But remember how statistics is prone to error? Even decisions about hypothesis testing are prone to error. A research that rejects their null hypothesis can be wrong (Whaaaaaat researchers can be wrong? What a concept.). Alpha levels are important to hypothesis testing because they tell us how often we are will reject the null hypothesis and be wrong. Psychological researchers often set alpha levels to .05, meaning that they are willing to accept that they are wrong 5% of the time. More accurately, they will reject the null hypothesis when it is actually true 5% of the time. 6.4 Type I &amp; Type II Errors There are actually two types of errors in hypothesis testing. Type I errors (\\(\\alpha\\)) happen when a researcher rejects the null hypothesis when it is actually true. Our alpha level reflects the rate of this type of error. In addition, Type II errors (\\(\\beta\\)) happen when a researcher fails to reject the null hypothesis when it is actually not true. I remember them by thinking to myself alpha is Type I because 1 comes first and alpha comes first… Which is worse? Psychology students are usually taught that Type I errors are typically worse. After all, if we incorrectly determine that a treatment is beneficial, when it is actually harmful, that would be a pretty serious error. On the flipside, if a treatment is actually beneficial but but our hypothesis test tells us that it’s not, then people may suffer without that treatment. A more nuanced view of this dilemma aks us to look at these things in context and ask more questions. What are the possible benefits? What are the possible consequences? 6.5 Power and Sample Size Just as Type I errors correspond to alpha levels, Type II errors correspond to power. Power is the rate at which we will correctly reject the null hypothesis. Power is actually \\(1 - Type\\ II\\ error\\ rate\\). The following table will help make sense of these concepts and how they are related: Reject the Null Fail to Reject the Null Null is True Type I Error(\\(\\alpha\\)) Correct Null is False Power(\\(1 - \\beta\\)) Type II Error(\\(\\beta\\)) 6.6 Directional and Non-Directional insert footnote here↩ "],
["single-sample-z-test.html", "Chapter 7 Single Sample Z-Test 7.1 Example Setup", " Chapter 7 Single Sample Z-Test Usage: To check for a difference between a sampe mean and a population mean. Requirements: One variable that is interval/ratio, a population mean, and a population standard error. Steps to conducting an single samples z-test: Write out null and alternative hypotheses Determine critical z-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate z-value for your data A. You must first find the sample mean, population mean, and standard error Compare your calculated z-value to the critical z-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (Cohen’s d) 7.1 Example Setup 7.1.1 Hypotheses \\(\\Large H_0: \\mu = value\\) \\(\\Large H_1: \\mu \\neq value\\) 7.1.2 Critical z-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 7.1.3 Calculate z \\(\\Large N = number\\ of\\ scores\\) \\(\\Large \\bar X = \\frac{sum}{N}\\) \\(\\Large \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{N}}\\) \\(\\Large z = \\frac{\\bar X - \\mu}{\\sigma_{\\bar X}}\\) 7.1.4 Compare Observed z to CV If z is greater than the CV, reject the null hypothesis. If z is less than the CV, fail to reject the null hypothesis. 7.1.5 Calculate Effect Size \\(\\Large d = \\frac{\\bar X - \\mu}{\\sigma}\\) "],
["single-sample-t-test.html", "Chapter 8 Single Sample T-Test 8.1 Example Setup", " Chapter 8 Single Sample T-Test Usage: To check for a difference between a sampe mean and a population mean when the population error is unknown. Requirements: One variable that is interval/ratio and a population mean. Steps to conducting an single samples t-test: Write out null and alternative hypotheses Determine critical t-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate t-value for your data A. You must first find the sample mean, population mean, and estimated standard error Compare your calculated t-value to the critical t-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (Cohen’s d) Calculate confidence interval (CI). 8.1 Example Setup 8.1.1 Hypotheses \\(\\Large H_0: \\mu = value\\) \\(\\Large H_1: \\mu \\neq value\\) 8.1.2 Critical t-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 8.1.3 Calculate t \\(\\Large N = number\\ of\\ scores\\) \\(\\Large \\bar X = \\frac{sum}{N}\\) \\(\\Large \\hat s_\\bar X = \\frac{\\hat s}{\\sqrt{N}}\\) \\(\\Large t = \\frac{\\bar X - \\mu}{\\hat s_\\bar X}\\) 8.1.4 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 8.1.5 Calculate Effect Size \\(\\Large d = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s^2_p}}\\) 8.1.6 Calculate Confidence Interval (CI) \\(\\Large CI = \\bar X_\\ \\pm\\ (t_{critical}* \\hat{s}_{\\bar X})\\) "],
["independent-samples-t-test.html", "Chapter 9 Independent Samples T-Test 9.1 Example Setup", " Chapter 9 Independent Samples T-Test Usage: To check for a difference between two group means. Requirements: One variable with two levels and a second variable that is interval/ratio. Steps to conducting an independent samples t-test: Write out null and alternative hypotheses Determine critical t-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate t-value for your data A. You must first find the sample mean, population mean, and estimated standard error Compare your calculated t-value to the critical t-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (eta squared \\(\\eta^2\\) or Cohen’s d) Calculate confidence interval (CI). 9.1 Example Setup 9.1.1 Hypotheses \\(\\Large H_0: \\mu_1 - \\mu_2 = 0\\) \\(\\Large H_1: \\mu_1 - \\mu_2 \\neq 0\\) 9.1.2 Critical t-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 9.1.3 Calculate t \\(\\Large n_{group} = number\\ of\\ scores\\ in\\ group\\) \\(\\Large \\bar X_{group} = \\frac{sum}{n}\\) \\(\\Large SS_{group} = \\sum X^2_{group} - \\frac{(\\sum X_{group})^2}{n_{group}}\\) Calculate the previous three for both groups. \\(\\Large \\hat{s}_{\\bar X_1 - \\bar X_2} = \\sqrt{(\\frac{SS_1 + SS_2}{n_1 + n_2 - 2})(\\frac{1}{n_1} + \\frac{1}{n_2})}\\) \\(\\Large t = \\frac{\\bar X_1 - \\bar X_2}{\\hat{s}_{\\bar X_1 - \\bar X_2}}\\) 9.1.4 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 9.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{t^2}{t^2 + df}\\) \\(\\Large d = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s^2_p}}\\) 9.1.6 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar X_1 - \\bar X_2)\\ \\pm\\ (t_{critical}* \\hat{s}_{\\bar X_1 - \\bar X_2})\\) "],
["repeated-measures-t-test.html", "Chapter 10 Repeated Measures T-Test 10.1 Example Setup", " Chapter 10 Repeated Measures T-Test Usage: To check for a difference between means from the same group. Requirements: Two variables that are interval/ratio. Steps to conducting an repeated measures t-test: Write out null and alternative hypotheses Determine critical t-value for rejection of null hypothesis A. Use degrees of freedom (df) and alpha level Calculate t-value for your data A. You must first find the mean difference and the estimated standard error. Compare your calculated t-value to the critical t-value A. If your calculated value is greater than the critical value, you reject the null hypothesis B. If your calculated value is less than the critical value, you fail to reject the null hypothesis Calculate effect size (eta squared \\(\\eta^2\\) or Cohen’s d) Calculate confidence interval (CI). 10.1 Example Setup 10.1.1 Hypotheses \\(\\Large H_0: \\mu_a = \\mu_b\\) \\(\\Large H_1: \\mu_a \\neq \\mu_b\\) 10.1.2 Critical t-Value (CV) \\(\\Large df = n - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 10.1.3 Calculate t \\(\\Large N = number\\ of\\ participants\\) \\(\\Large \\bar D = \\bar X_a - \\bar X_b\\) \\(\\Large SS_{D} = \\sum D^2 - \\frac{(\\sum D)^2}{N}\\) \\(\\Large \\hat{s}_D^2 = \\frac{SS_D}{N - 1}\\) \\(\\Large \\hat{s}_D = \\sqrt{\\hat{s}_D^2}\\) \\(\\Large \\hat{s}_{\\bar D} = \\frac{\\hat{s}_D}{\\sqrt{N}}\\) \\(\\Large t = \\frac{\\bar D}{\\hat{s}_{\\bar D}}\\) 10.1.4 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 10.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{t^2}{t^2 + df}\\) \\(\\Large d = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s^2_p}}\\) 10.1.6 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar D)\\ \\pm\\ (t_{critical}* \\hat{s}_{\\bar D})\\) "],
["one-way-between-subjects-anova.html", "Chapter 11 One-Way Between-Subjects ANOVA 11.1 Example Setup 11.2 Tips", " Chapter 11 One-Way Between-Subjects ANOVA Usage: To check for a difference between three or more group means. Requirements: Three or more variables that are interval/ratio. Note: This is basically the same as the independent groups t-test, but with more than two means. Steps to conducting an one-way between-subjects ANOVA: Write out null and alternative hypotheses. Determine critical F-value for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate F-value for your data. A. You must first find the sums of squares and mean squares. Compare your calculated F-value to the critical F-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (eta squared \\(\\eta^2\\)). Run post hoc tests if neccessary. Calculate confidence interval (CI). 11.1 Example Setup 11.1.1 Hypotheses \\(\\Large H_0: \\mu_a = \\mu_b = \\mu_c\\) \\(\\Large H_1: the\\ three\\ means\\ are\\ not\\ equal\\) 11.1.2 Critical F-Value (CV) \\(\\Large N = number\\ of\\ participants\\) \\(\\Large n_{group} = number\\ of\\ participants\\ in\\ group\\) \\(\\Large k = number\\ of\\ groups\\) \\(\\Large df_{between} = k - 1\\) \\(\\Large df_{within} = N - k\\) \\(\\Large df_{total} = N - 1\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 11.1.3 Calculate F \\(\\Large T_{group} = \\sum X_{group}\\) \\(\\Large SS_{total} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{within} = \\sum X^2 - \\frac{\\sum T^2}{n}\\) \\(\\Large SS_{between} = \\frac{\\sum T^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large MS_{between} = \\frac{SS_{between}}{df_{between}}\\) \\(\\Large MS_{within} = \\frac{SS_{within}}{df_{within}}\\) \\(\\Large F = \\frac{MS_{between}}{MS_{within}}\\) 11.1.4 Compare Observed F to CV If F is greater than the CV, reject the null hypothesis. If F is less than the CV, fail to reject the null hypothesis. 11.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{SS_{between}}{SS_{total}}\\) 11.1.6 Post Hoc Testing Which means are different? Run Tukey’s HSD Test. \\(\\Large q = [check\\ table]\\) \\(\\Large CD = q\\sqrt{\\frac{MS_{within}}{n}}\\) Check to see if any of the differences between the means is greater than the CD. 11.1.7 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar X_a - \\bar X_b)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_a - \\bar X_c)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_b - \\bar X_c)\\ \\pm\\ CD\\) 11.2 Tips Draw a table like the one below to organize your calculations as you go. Source SS df MS F Between 50 2 25 12.5 Within 30 15 2 - Total 80 17 - - F is actually closely related to t: \\(\\Large F = t^2\\) "],
["one-way-repeated-measures-anova.html", "Chapter 12 One-Way Repeated Measures ANOVA 12.1 Example Setup 12.2 Tips", " Chapter 12 One-Way Repeated Measures ANOVA Usage: To check for a difference between three or more group means. Requirements: Three or more variables that are interval/ratio and within-subject design. Note: This is basically the same as the repeated measures t-test, but with more than two means. Steps to conducting an one-way repeated measures ANOVA: Write out null and alternative hypotheses. Determine critical F-value for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate F-value for your data. A. You must first find the sums of squares and mean squares. Compare your calculated F-value to the critical F-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (eta squared \\(\\eta^2\\)). Run post hoc tests if neccessary. Calculate confidence interval (CI). 12.1 Example Setup 12.1.1 Hypotheses \\(\\Large H_0: \\mu_a = \\mu_b = \\mu_c\\) \\(\\Large H_1: the\\ three\\ means\\ are\\ not\\ equal\\) 12.1.2 Critical F-Value (CV) \\(\\Large N = number\\ of\\ scores\\) \\(\\Large n = number\\ of\\ participants\\) \\(\\Large k = number\\ of\\ conditions\\) \\(\\Large df_{between\\ conditions} = k - 1\\) \\(\\Large df_{total} = N - 1\\) \\(\\Large df_{between\\ subjects} = n - 1\\) \\(\\Large df_{error} = (k-1)*(n-1)\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 12.1.3 Calculate F \\(\\Large T_{condition} = \\sum X_{condition}\\) \\(\\Large P_i = sum\\ of\\ scores\\ for\\ participant\\ i\\) \\(\\Large SS_{total} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{between\\ conditions} = \\frac{\\sum T^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{across\\ subjects} = \\frac{\\sum P_i^2}{k} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{error} = SS_{total} - SS_{between\\ conditions} - SS_{across\\ subjects}\\) \\(\\Large MS_{between\\ conditions} = \\frac{SS_{between\\ conditions}}{df_{between\\ conditions}}\\) \\(\\Large MS_{error} = \\frac{SS_{error}}{df_{error}}\\) \\(\\Large F = \\frac{MS_{between\\ conditions}}{MS_{error}}\\) 12.1.4 Compare Observed F to CV If F is greater than the CV, reject the null hypothesis. If F is less than the CV, fail to reject the null hypothesis. 12.1.5 Calculate Effect Size \\(\\Large \\eta^2 = \\frac{SS_{between\\ conditions}}{SS_{between\\ conditions} + SS_{error}}\\) 12.1.6 Post Hoc Testing Which means are different? Run Tukey’s HSD Test. \\(\\Large q = [check\\ table]\\) \\(\\Large CD = q\\sqrt{\\frac{MS_{error}}{n}}\\) Check to see if any of the differences between the means is greater than the CD. 12.1.7 Calculate Confidence Interval (CI) \\(\\Large CI = (\\bar X_a - \\bar X_b)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_a - \\bar X_c)\\ \\pm\\ CD\\) \\(\\Large CI = (\\bar X_b - \\bar X_c)\\ \\pm\\ CD\\) 12.2 Tips Draw a table like the one below to organize your calculations as you go. Source SS df MS F Between Conditions 40 2 20 2 Error 20 2 10 - Across Subjects 80 2 - - Total 140 6 F is actually closely related to t: \\(\\Large F = t^2\\) "],
["two-way-anova.html", "Chapter 13 Two-Way ANOVA 13.1 Example Setup 13.2 Tips", " Chapter 13 Two-Way ANOVA Usage: To check for a difference between three or more group means with two sets of factors. Requirements: Three or more variables that are interval/ratio and two grouping variables. Steps to conducting an two-way ANOVA: Write out null and alternative hypotheses. A. Main effects of Factor A. B. Main effects of Factor B. C. Interaction of Factors A and B. Determine critical F-values for rejection of null hypotheses. A. Use degrees of freedom (df) and alpha level . B. One CV for each hypothesis. Calculate 3 F-values for your data. A. One for each hypothesis. Compare your calculated F-value to the critical F-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (eta squared \\(\\eta^2\\)). 13.1 Example Setup 13.1.1 Hypotheses For Factor A: \\(\\Large H_0: \\mu_{a_1} = \\mu_{a_2}\\) \\(\\Large H_1: \\mu_{a_1} \\neq \\mu_{a_2}\\) For Factor B: \\(\\Large H_0: \\mu_{b_1} = \\mu_{b_2}\\) \\(\\Large H_1: \\mu_{b_1} \\neq \\mu_{b_2}\\) For Interaction: \\(\\Large H_0: No\\ interaction\\) \\(\\Large H_1: Interaction\\ present\\) 13.1.2 Critical F-Value (CV) \\(\\Large N = number\\ of\\ participants\\) \\(\\Large n_{group} = number\\ of\\ participants\\ in\\ group\\) \\(\\Large k = number\\ of\\ groups\\) \\(\\Large df_{between} = k - 1\\) \\(\\Large df_{within} = (levels\\ of\\ A)*(levels\\ of\\ B)(n - 1)\\) \\(\\Large df_{total} = N - 1\\) \\(\\Large df_{A} = (levels\\ of\\ A) - 1\\) \\(\\Large df_{B} = (levels\\ of\\ B) - 1\\) \\(\\Large df_{interaction} = (df_A)*(df_B)\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 13.1.3 Calculate F \\(\\Large T_{group} = \\sum X_{group}\\) \\(\\Large SS_{total} = \\sum X^2 - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{within} = \\sum X^2 - \\frac{\\sum T^2}{n}\\) \\(\\Large SS_{between} = \\frac{\\sum T^2}{n} - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{A} = \\sum(\\frac{T^2_{row}}{n_{row}}) - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{B} = \\sum(\\frac{T^2_{col}}{n_{col}}) - \\frac{(\\sum X)^2}{N}\\) \\(\\Large SS_{AB} = SS_{between} - SS_A - SS_B\\) \\(\\Large MS_{A} = \\frac{SS_{A}}{df_{A}}\\) \\(\\Large MS_{B} = \\frac{SS_{B}}{df_{B}}\\) \\(\\Large MS_{AB} = \\frac{SS_{AB}}{df_{AB}}\\) \\(\\Large F_A = \\frac{MS_{A}}{MS_{within}}\\) \\(\\Large F_B = \\frac{MS_{B}}{MS_{within}}\\) \\(\\Large F_{AB} = \\frac{MS_{AB}}{MS_{within}}\\) 13.1.4 Compare Observed F-Value to CVs If F is greater than the CV, reject the null hypothesis. If F is less than the CV, fail to reject the null hypothesis. 13.1.5 Calculate Effect Size \\(\\Large \\eta^2_A = \\frac{SS_{A}}{SS_{A} + SS_{within}}\\) \\(\\Large \\eta^2_B = \\frac{SS_{B}}{SS_{B} + SS_{within}}\\) \\(\\Large \\eta^2_{AB} = \\frac{SS_{AB}}{SS_{AB} + SS_{within}}\\) 13.2 Tips Draw a table like the one below to organize your calculations as you go. Source SS df MS F Main Effect Factor A 40 1 40 2 Main Effect Factor B 40 1 40 2 Interaction 80 1 80 4 Within 140 7 20 Total 300 10 F is actually closely related to t: \\(\\Large F = t^2\\) "],
["correlation-and-regression.html", "Chapter 14 Correlation and Regression 14.1 Example Setup 14.2 Tips", " Chapter 14 Correlation and Regression Usage: To check for a relationship between variables. Requirements: Two or more variables that are interval/ratio. Steps to calculating a correlation coefficient: Write out null and alternative hypotheses. Calculate r-value (correlation coefficient). Determine critical t-value (yep!) for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate t-value. Compare your calculated t-value to the critical t-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (\\(r^2\\)). Estimate regression equation. Test significance of regression equation. A. Run an ANOVA. 14.1 Example Setup 14.1.1 Hypotheses \\(\\Large H_0: \\rho = 0\\) \\(\\Large H_1: \\rho \\neq 0\\) 14.1.2 Calculate r \\(\\Large n = number\\ of\\ participants\\) \\(\\Large SS_{X} = \\sum X^2 - \\frac{(\\sum X)^2}{n}\\) \\(\\Large SS_{Y} = \\sum Y^2 - \\frac{(\\sum Y)^2}{n}\\) \\(\\Large SP = \\sum XY - \\frac{\\sum X * \\sum Y}{n}\\) \\(\\Large r = \\frac{SP}{\\sqrt{SS_X * SS_Y}}\\) 14.1.3 Determine Critical t-Value (CV) \\(\\Large df = n - 2\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 14.1.4 Calculate t \\(\\Large t = \\frac{r}{\\sqrt{\\frac{(1 - r^2}{(n - 2)}})}\\) 14.1.5 Compare Observed t to CV If t is greater than the CV, reject the null hypothesis. If t is less than the CV, fail to reject the null hypothesis. 14.1.6 Calculate Effect Size \\(\\Large r^2 = r * r\\) 14.1.7 Estimate Regression Equation Regression line equation: \\(\\hat Y = bX + a\\) \\(\\Large b = \\frac{SP}{SS_X}\\) \\(\\Large a = \\bar Y - (b*\\bar X)\\) \\(\\Large SEoE = \\sqrt{\\frac{(1 - r^2)*SS_Y}{n - 2}}\\) 14.1.8 Testing Significance of Regression 14.1.8.1 Hypotheses \\(\\Large H_0: b = 0\\) \\(\\Large H_1: b \\neq 0\\) 14.1.8.2 Run an ANOVA \\(\\Large df_{regression} = 1\\) \\(\\Large df_{residual} = n - 2\\) \\(\\Large SS_{regression} = r^2*SS_Y\\) \\(\\Large SS_{residual} = (1 - r^2)*SS_Y\\) \\(\\Large MS_{regression} = \\frac{SS_{regression}}{df_{regression}}\\) \\(\\Large MS_{residual} = \\frac{SS_{residual}}{df_{residual}}\\) \\(\\Large F = \\frac{MS_{regression}}{MS_{residual}}\\) 14.1.8.3 Check significance \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 14.2 Tips Remember that a t-test is used when hypothesis testing correlation coefficients! Remember that \\(df = n - 2\\) for tests of correlation coefficients! "],
["chi-square-test.html", "Chapter 15 Chi-Square Test 15.1 Example Setup 15.2 Chi-Square Goodness-of-Fit Test 15.3 Tips", " Chapter 15 Chi-Square Test Usage: To check for a relationship between variables. Requirements: Two nominal variables. Steps to running a chi-square test: Write out null and alternative hypotheses. Calculate \\(\\chi^2\\)-value. Determine critical \\(\\chi^2\\)-value (yep!) for rejection of null hypothesis. A. Use degrees of freedom (df) and alpha level. Calculate \\(\\chi^2\\)-value. Compare your calculated \\(\\chi^2\\)-value to the critical \\(\\chi^2\\)-value. A. If your calculated value is greater than the critical value, you reject the null hypothesis. B. If your calculated value is less than the critical value, you fail to reject the null hypothesis. Calculate effect size (Cramer’s V). 15.1 Example Setup 15.1.1 Hypotheses \\(\\Large H_0: X\\ and\\ Y\\ are\\ unrelated\\) \\(\\Large H_1: X\\ and\\ Y\\ are\\ related\\) 15.1.2 Calculate Chi-Square \\(\\Large N = number\\ of\\ participants\\) \\(\\Large O = observed\\ values\\ in\\ each\\ cell\\) \\(\\Large CMF_{cell} = \\sum values\\ in\\ column\\) \\(\\Large RMF_{cell} = \\sum values\\ in\\ row\\) \\(\\Large E_{cell} = (\\frac{CMF_{cell}}{N})*(RMF_{cell})\\) \\(\\Large \\chi^2 = \\sum \\frac{(O_{cell} - E_{cell})^2}{E_{cell}}\\) 15.1.3 Determine Critical Chi-Square Value (CV) \\(\\Large df = (rows - 1)(columns - 1)\\) \\(\\Large \\alpha = .05\\) \\(\\Large CV = [check\\ table]\\) 15.1.4 Compare Observed Chi-Square to CV If \\(\\chi^2\\) is greater than the CV, reject the null hypothesis. If \\(\\chi^2\\) is less than the CV, fail to reject the null hypothesis. 15.1.5 Calculate Effect Size \\(\\Large L = number\\ of\\ levels\\ of\\ variable\\ with\\ least\\ levels\\) \\(\\Large V = \\sqrt{\\frac{\\chi^2}{N*(L - 1)}}\\) 15.2 Chi-Square Goodness-of-Fit Test This involves testing the relative frequencies of a single nominal variables. The setup is similar to a general \\(\\chi^2\\) test. The main difference is the calculation of the expected frequencies. \\(\\Large rf_{category} = given\\) Or, if it is not given and all categories are expected to be proportional: \\(\\Large rf_{category} = \\frac{N}{number\\ of\\ categories}\\) \\(\\Large E_{category} = (rf_{category})*(N)\\) 15.3 Tips \\(\\chi^2\\) is pronounced K-Y, NOT CH-Y. Remember not to get \\(\\chi^2\\) mixed up with X. Draw a table like the one below to organize your calculations as you go. Cell \\(O\\) \\(E\\) \\(O - E\\) \\((O - E)^2\\) \\(\\frac{(O - E)^2}{E}\\) Religious Males 20 23.08 -3.08 9.49 0.41 Religious Females 30 26.92 3.08 9.49 0.35 Non-Religious Males 10 6.92 3.08 9.49 1.37 Non-Religious Females 5 8.08 -3.08 9.49 1.17 \\(\\chi^2\\) 3.30 "],
["references.html", "Chapter 16 References", " Chapter 16 References Jaccard, J. &amp; Becker, M. A. (2002). Statistics for the behavioral sciences. Wadsworth Group. Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677-680. "]
]
